#!/bin/bash

# Nerdbuntu Export Script
# Bundles all processed markdown files and ChromaDB vector database for transport

set -e

echo "=== Nerdbuntu Export Script ==="
echo "This script bundles your RAG data for transport to another machine"
echo ""

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Project directory
PROJECT_DIR="$HOME/nerdbuntu"
OUTPUT_DIR="$PROJECT_DIR/data/output"
VECTOR_DB_DIR="$PROJECT_DIR/data/vector_db"
EXPORT_DIR="$PROJECT_DIR/exports"

# Check if project directory exists
if [ ! -d "$PROJECT_DIR" ]; then
    echo -e "${RED}Error: Nerdbuntu project directory not found at $PROJECT_DIR${NC}"
    echo "Please run setup.sh first"
    exit 1
fi

# Create exports directory if it doesn't exist
mkdir -p "$EXPORT_DIR"

# Generate timestamp for unique export filename
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
EXPORT_NAME="nerdbuntu_rag_export_${TIMESTAMP}"
EXPORT_PATH="$EXPORT_DIR/${EXPORT_NAME}.zip"

echo -e "${GREEN}Step 1: Checking data directories...${NC}"

# Check if output directory has files
if [ ! -d "$OUTPUT_DIR" ] || [ -z "$(ls -A $OUTPUT_DIR 2>/dev/null)" ]; then
    echo -e "${YELLOW}Warning: No files found in output directory ($OUTPUT_DIR)${NC}"
    echo "Have you processed any PDFs yet?"
    read -p "Continue anyway? (y/n): " response
    if [[ ! "$response" =~ ^[Yy]$ ]]; then
        exit 0
    fi
    OUTPUT_FILES=0
else
    OUTPUT_FILES=$(find "$OUTPUT_DIR" -type f | wc -l)
    echo -e "${BLUE}Found $OUTPUT_FILES file(s) in output directory${NC}"
fi

# Check if vector database exists
if [ ! -d "$VECTOR_DB_DIR" ] || [ -z "$(ls -A $VECTOR_DB_DIR 2>/dev/null)" ]; then
    echo -e "${YELLOW}Warning: Vector database directory is empty ($VECTOR_DB_DIR)${NC}"
    read -p "Continue anyway? (y/n): " response
    if [[ ! "$response" =~ ^[Yy]$ ]]; then
        exit 0
    fi
    VECTOR_DB_SIZE="0 bytes"
else
    VECTOR_DB_SIZE=$(du -sh "$VECTOR_DB_DIR" | cut -f1)
    echo -e "${BLUE}Vector database size: $VECTOR_DB_SIZE${NC}"
fi

echo ""
echo -e "${GREEN}Step 2: Creating temporary export structure...${NC}"

# Create temporary directory for organizing export
TEMP_DIR=$(mktemp -d)
TEMP_EXPORT="$TEMP_DIR/$EXPORT_NAME"
mkdir -p "$TEMP_EXPORT"

# Copy output files
echo "Copying markdown files..."
if [ -d "$OUTPUT_DIR" ] && [ "$(ls -A $OUTPUT_DIR 2>/dev/null)" ]; then
    mkdir -p "$TEMP_EXPORT/markdown"
    cp -r "$OUTPUT_DIR"/* "$TEMP_EXPORT/markdown/" 2>/dev/null || true
fi

# Copy vector database
echo "Copying vector database..."
if [ -d "$VECTOR_DB_DIR" ] && [ "$(ls -A $VECTOR_DB_DIR 2>/dev/null)" ]; then
    mkdir -p "$TEMP_EXPORT/vector_db"
    cp -r "$VECTOR_DB_DIR"/* "$TEMP_EXPORT/vector_db/" 2>/dev/null || true
fi

# Create metadata file
echo -e "${GREEN}Step 3: Creating metadata...${NC}"
cat > "$TEMP_EXPORT/EXPORT_INFO.txt" << EOF
Nerdbuntu RAG Export
====================

Export Date: $(date)
Export Machine: $(hostname)
Export User: $(whoami)

Contents:
---------
- Markdown Files: $OUTPUT_FILES files
- Vector Database Size: $VECTOR_DB_SIZE
- Export Name: $EXPORT_NAME

Directory Structure:
--------------------
$EXPORT_NAME/
â”œâ”€â”€ markdown/          # Processed markdown files with semantic metadata
â”œâ”€â”€ vector_db/         # ChromaDB vector database
â”œâ”€â”€ EXPORT_INFO.txt    # This file
â””â”€â”€ IMPORT_README.md   # Instructions for importing on destination machine

Vector Database Info:
---------------------
- Database Type: ChromaDB
- Collection Name: markdown_chunks
- Embedding Model: all-MiniLM-L6-v2
- Similarity Metric: Cosine

Next Steps:
-----------
1. Extract this archive on your destination machine
2. Follow instructions in IMPORT_README.md
3. Update configuration with your Azure credentials
4. Start querying your RAG data!

Generated by: Nerdbuntu Export Script
Repository: https://github.com/Cosmicjedi/nerdbuntu
EOF

# Create import instructions
cat > "$TEMP_EXPORT/IMPORT_README.md" << 'EOF'
# Importing Nerdbuntu RAG Data

This archive contains your processed markdown files and vector database ready for RAG applications.

## Quick Import (Same Machine Type - Ubuntu/Linux)

### Option 1: Import into Nerdbuntu Installation

If you have Nerdbuntu installed on the destination machine:

```bash
# 1. Extract the archive
unzip nerdbuntu_rag_export_*.zip
cd nerdbuntu_rag_export_*

# 2. Copy to Nerdbuntu data directories
cp -r markdown/* ~/nerdbuntu/data/output/
cp -r vector_db/* ~/nerdbuntu/data/vector_db/

# 3. Start using
cd ~/nerdbuntu
source venv/bin/activate
python app.py  # GUI will show all imported files
# or
python examples.py query "your search query"
```

### Option 2: Standalone RAG Application

Use the data in your own RAG application:

```python
import chromadb
from sentence_transformers import SentenceTransformer

# Initialize ChromaDB with imported database
client = chromadb.PersistentClient(path="./vector_db")
collection = client.get_collection("markdown_chunks")

# Initialize embedding model (must match export)
model = SentenceTransformer('all-MiniLM-L6-v2')

# Query the database
def query_rag(question, n_results=5):
    query_embedding = model.encode([question])[0]
    results = collection.query(
        query_embeddings=[query_embedding.tolist()],
        n_results=n_results
    )
    return results

# Example usage
results = query_rag("What is machine learning?")
for doc in results['documents'][0]:
    print(doc)
    print("---")
```

## Contents

- **markdown/**: Processed markdown files with:
  - Original content from PDFs
  - Semantic metadata
  - Key concepts
  - Backlinks section

- **vector_db/**: ChromaDB database containing:
  - Vector embeddings for all text chunks
  - Metadata for each chunk
  - Pre-computed similarities

## Dependencies

To use this data, you need:

```bash
pip install chromadb sentence-transformers
```

## Important Notes

1. **Embedding Model**: The vectors were created with `all-MiniLM-L6-v2`. You must use the same model for queries.

2. **ChromaDB Version**: Generated with ChromaDB 0.4.x+. May need adjustment for older versions.

3. **Markdown Files**: Can be used independently or with the vector database.

4. **Azure Credentials**: If using Nerdbuntu features (concept extraction, etc.), configure your Azure credentials in .env file.

## Integration Examples

### With LangChain

```python
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')
vectorstore = Chroma(
    persist_directory="./vector_db",
    embedding_function=embeddings,
    collection_name="markdown_chunks"
)

# Use with retrieval chain
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
```

### With LlamaIndex

```python
from llama_index import VectorStoreIndex
from llama_index.vector_stores import ChromaVectorStore
from llama_index.storage.storage_context import StorageContext
import chromadb

chroma_client = chromadb.PersistentClient(path="./vector_db")
chroma_collection = chroma_client.get_collection("markdown_chunks")

vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

index = VectorStoreIndex.from_vector_store(
    vector_store=vector_store,
    storage_context=storage_context
)
```

## Troubleshooting

**Error: Collection not found**
- Ensure you extracted the entire `vector_db` directory
- Check that ChromaDB version is compatible (0.4.x+)

**Error: Embedding dimension mismatch**
- You must use `all-MiniLM-L6-v2` model for queries
- This model produces 384-dimensional embeddings

**Performance Issues**
- ChromaDB performs best with SSD storage
- Consider increasing `n_results` parameter if queries are slow

## Support

For questions or issues:
- GitHub Issues: https://github.com/Cosmicjedi/nerdbuntu/issues
- Documentation: https://github.com/Cosmicjedi/nerdbuntu

---

**Happy RAG building! ðŸš€**
EOF

echo -e "${GREEN}Step 4: Creating ZIP archive...${NC}"
cd "$TEMP_DIR"
zip -r "$EXPORT_PATH" "$EXPORT_NAME" -q

# Get final archive size
ARCHIVE_SIZE=$(du -sh "$EXPORT_PATH" | cut -f1)

# Cleanup temp directory
rm -rf "$TEMP_DIR"

echo ""
echo -e "${GREEN}=== Export Complete! ===${NC}"
echo ""
echo -e "${BLUE}Export Summary:${NC}"
echo "  Archive: $EXPORT_PATH"
echo "  Size: $ARCHIVE_SIZE"
echo "  Markdown files: $OUTPUT_FILES"
echo "  Vector DB size: $VECTOR_DB_SIZE"
echo ""
echo -e "${YELLOW}Next Steps:${NC}"
echo "  1. Transfer the archive to your destination machine:"
echo "     scp $EXPORT_PATH user@destination:/path/"
echo ""
echo "  2. Extract and follow IMPORT_README.md instructions"
echo ""
echo -e "${GREEN}Archive contents:${NC}"
unzip -l "$EXPORT_PATH" | head -n 20
echo ""
echo -e "${BLUE}Full path to export:${NC}"
echo "  $EXPORT_PATH"
echo ""
echo -e "${GREEN}Export ready for transport! ðŸš€${NC}"
