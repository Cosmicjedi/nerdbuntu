# Multi-Document Processing Guide

## ğŸ¯ Short Answer

**YES!** Each PDF you process:
- âœ… Creates its own `.md` file in the output folder
- âœ… Adds ALL its chunks to the SHARED vector database
- âœ… Can be queried across all documents

## ğŸ“š How It Works

### Using the GUI

Process PDFs one at a time:

```
1st PDF â†’ output/document1.md + 23 chunks added to database
2nd PDF â†’ output/document2.md + 18 chunks added to database (now 41 total)
3rd PDF â†’ output/document3.md + 31 chunks added to database (now 72 total)
4th PDF â†’ output/document4.md + 27 chunks added to database (now 99 total)
```

**Each document:**
- Gets its own markdown file
- Contributes to ONE shared vector database
- Can reference content from other documents

### Vector Database Structure

```python
# ChromaDB stores everything with unique IDs:
Vector Database:
â”œâ”€â”€ document1.pdf_chunk_0  (from first PDF)
â”œâ”€â”€ document1.pdf_chunk_1
â”œâ”€â”€ document1.pdf_chunk_2
â”œâ”€â”€ ...
â”œâ”€â”€ document2.pdf_chunk_0  (from second PDF)
â”œâ”€â”€ document2.pdf_chunk_1
â”œâ”€â”€ ...
â”œâ”€â”€ document3.pdf_chunk_0  (from third PDF)
â””â”€â”€ ...
```

## ğŸš€ Processing Multiple Documents

### Method 1: GUI (One at a Time)

1. Launch GUI: `./launch_gui.sh`
2. Select first PDF â†’ Process
3. Wait for completion
4. Select second PDF â†’ Process
5. Repeat for each document

**Pros:**
- Visual feedback
- Easy to use
- See detailed progress

**Cons:**
- One at a time
- Manual clicking

### Method 2: CLI Batch Processing (Recommended for Many Files)

Process ALL PDFs in a folder at once:

```bash
# Process all PDFs in a folder
python3 multi_document_demo.py process folder/*.pdf -o output/

# Or list specific files
python3 multi_document_demo.py process doc1.pdf doc2.pdf doc3.pdf
```

**Example output:**
```
======================================================================
Processing 4 PDF files
Output directory: output
Semantic mode: ENABLED
======================================================================

[1/4] Processing: financial_report.pdf
----------------------------------------------------------------------
  Converting PDF to Markdown...
  âœ“ Converted (45821 characters)
  Applying semantic processing...
  âœ“ Semantic processing complete
  âœ“ Saved to: output/financial_report.md

[2/4] Processing: technical_spec.pdf
----------------------------------------------------------------------
  Converting PDF to Markdown...
  âœ“ Converted (32156 characters)
  Applying semantic processing...
  âœ“ Semantic processing complete
  âœ“ Saved to: output/technical_spec.md

[3/4] Processing: user_manual.pdf
----------------------------------------------------------------------
  Converting PDF to Markdown...
  âœ“ Converted (28943 characters)
  Applying semantic processing...
  âœ“ Semantic processing complete
  âœ“ Saved to: output/user_manual.md

[4/4] Processing: meeting_notes.pdf
----------------------------------------------------------------------
  Converting PDF to Markdown...
  âœ“ Converted (12458 characters)
  Applying semantic processing...
  âœ“ Semantic processing complete
  âœ“ Saved to: output/meeting_notes.md

======================================================================
PROCESSING COMPLETE
======================================================================

âœ“ Successfully processed: 4/4

Output files:
  âœ“ output/financial_report.md
  âœ“ output/technical_spec.md
  âœ“ output/user_manual.md
  âœ“ output/meeting_notes.md

ğŸ“Š Vector Database Statistics:
  Total chunks stored: 99
  From 4 documents
  Average: 24 chunks per document
```

## ğŸ” Querying Across All Documents

Once you've processed multiple documents, you can search across ALL of them:

```bash
# Search for a topic
python3 multi_document_demo.py query "revenue projections"
```

**Example output:**
```
======================================================================
Searching for: 'revenue projections'
======================================================================

Found 5 similar chunks:

Result 1:
  Source: financial_report.pdf
  Chunk ID: 5
  Similarity: 87.3%
  Preview: ## Q4 Revenue Projections
  Based on current market trends, we project a 15% increase...

Result 2:
  Source: meeting_notes.pdf
  Chunk ID: 2
  Similarity: 78.9%
  Preview: Discussion point: Revenue forecast for next quarter
  Team consensus is optimistic based on...

Result 3:
  Source: financial_report.pdf
  Chunk ID: 12
  Similarity: 72.1%
  Preview: Historical revenue data shows steady growth pattern...
```

Notice how it finds relevant content from DIFFERENT documents!

## ğŸ“Š Database Statistics

Check what's in your database:

```bash
python3 multi_document_demo.py stats
```

**Example output:**
```
======================================================================
VECTOR DATABASE STATISTICS
======================================================================

Total chunks: 99
Database location: /home/user/nerdbuntu/data/vector_db

Documents: 4

Document list:
  - financial_report.pdf: 23 chunks
  - technical_spec.pdf: 31 chunks
  - user_manual.pdf: 27 chunks
  - meeting_notes.pdf: 18 chunks
```

## ğŸ’¡ Real-World Use Cases

### Use Case 1: Research Papers
```bash
# Process all your research papers
python3 multi_document_demo.py process research/*.pdf

# Find papers discussing "neural networks"
python3 multi_document_demo.py query "neural networks" -n 10
```

### Use Case 2: Company Documentation
```bash
# Process all company docs
python3 multi_document_demo.py process \
  employee_handbook.pdf \
  policies.pdf \
  procedures.pdf \
  guidelines.pdf

# Search across all docs
python3 multi_document_demo.py query "vacation policy"
```

### Use Case 3: Technical Manuals
```bash
# Process product manuals
python3 multi_document_demo.py process manuals/*.pdf

# Find troubleshooting info
python3 multi_document_demo.py query "error code 404"
```

## ğŸ¨ What Gets Created

### File System
```
output/
â”œâ”€â”€ document1.md      â† Individual markdown files
â”œâ”€â”€ document2.md
â”œâ”€â”€ document3.md
â””â”€â”€ document4.md

~/nerdbuntu/data/vector_db/
â””â”€â”€ [ChromaDB files]  â† Shared vector database
```

### Each Markdown File Contains

```markdown
---
source: document1.pdf
processed: 2025-10-06T16:30:45.123456
key_concepts: revenue, projections, Q4, analysis, forecast
chunks: 23
---

[Original PDF content converted to Markdown]

---

## Semantic Backlinks

This document is semantically linked in the vector database.
- **Key Concepts**: revenue, projections, Q4, analysis, forecast
- **Total Chunks**: 23
```

## ğŸ”„ Adding More Documents Later

You can ALWAYS add more documents:

```bash
# Process initial documents
python3 multi_document_demo.py process doc1.pdf doc2.pdf

# Later: Add more documents
python3 multi_document_demo.py process doc3.pdf doc4.pdf doc5.pdf

# They all go into the SAME database!
```

The vector database just keeps growing!

## âš ï¸ Important Notes

### 1. Unique Filenames
If you process `document.pdf` twice, the second one will:
- **Overwrite** the markdown file
- **Add new chunks** to the database (with same IDs, so it updates them)

### 2. Database Persistence
The vector database is **permanent** (stored on disk at `~/nerdbuntu/data/vector_db/`)
- Survives application restarts
- Keeps all your documents
- Can query anytime

### 3. Semantic Mode Required
For cross-document querying, you MUST have semantic features enabled:
- Needs Azure configuration
- Without it, you just get individual markdown files

### 4. Processing Time
- **Basic mode**: ~10 seconds per document
- **Semantic mode**: ~1-3 minutes per document
- Batch processing is faster than GUI one-by-one

## ğŸ¯ Summary

| Feature | GUI | CLI Batch |
|---------|-----|-----------|
| Multiple PDFs | âœ… One at a time | âœ… All at once |
| Markdown Output | âœ… Yes | âœ… Yes |
| Vector Database | âœ… Accumulates | âœ… Accumulates |
| Progress Display | âœ… Detailed | âœ… Per file |
| Cross-document Query | âœ… Yes | âœ… Yes |
| Speed | ğŸŒ Slower | ğŸš€ Faster |

**Bottom line:** Process as many PDFs as you want - they all accumulate in one searchable database! ğŸ‰
